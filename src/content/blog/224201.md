---
title: 'Un estudio detecta debilidades en la evaluación de los sistemas de IA'
description: 'La mayor revisión sistemática de benchmarks concluye que faltan criterios claros y estándares científicos rigurosos para medir el progreso de la inteligencia artificial.'
pubDate: '2025-11-08T14:18:22Z'
heroImage: ''
---

El Oxford Internet Institute (OII) acaba de poner el dedo en la llaga: no sabemos medir bien la inteligencia artificial. En un trabajo que ha reunido a 42 investigadores de centros de referencia —desde EPFL hasta Stanford—, el equipo ha analizado los  benchmarks más citados de la última década y ha descubierto fisuras preocupantes.  
Según el estudio, menos del 10 % de las pruebas especifica con precisión qué capacidad cognitiva pretende evaluar y solo uno de cada cinco artículos declara todas las variables de configuración del modelo. Esto significa que, a menudo, cuando se proclama que un sistema supera «el nivel humano», no queda claro ni qué habilidad concreta se ha medido ni si la comparación es justa.

Los autores denuncian una «crisis de reproducibilidad soterrada». Muchas bases de datos se actualizan sin control de versiones, se mezclan ejemplos de distinta calidad y, en ocasiones, ni siquiera se publican los conjuntos de prueba completos por motivos de copyright. El resultado es que pequeñas mejoras pueden deberse a trucos de ingeniería, a fugas de datos o a puro azar, y no a un avance genuino en la comprensión del lenguaje o la visión por computador.

Para frenar esta deriva, el OII propone un registro público de benchmarks con metadatos obligatorios, definiciones operativas estandarizadas de las tareas y auditorías independientes. Además, sugiere introducir métricas que penalicen el uso excesivo de datos y energía, para incentivar modelos más eficientes y sostenibles.

El informe llega en un momento clave: reguladores europeos y estadounidenses estudian cómo certificar sistemas de IA de alto riesgo. Sin criterios robustos de evaluación, advierten los autores, es imposible garantizar la seguridad, la equidad o la transparencia que exigen las nuevas leyes. El estudio, en definitiva, no solo revela fallos metodológicos; invita a replantear la cultura de la competición por el «state of the art» y a priorizar la ciencia abierta y la responsabilidad social en el desarrollo de la inteligencia artificial.

https://www.oii.ox.ac.uk/news-events/study-identifies-weaknesses-in-how-ai-systems-are-evaluated/